{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python391jvsc74a57bd063fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d",
   "display_name": "Python 3.9.1 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0       a\n1       b\n2       c\n3    aaba\n4     dog\n5     cat\ndtype: object\n0       A\n1       B\n2       C\n3    AABA\n4     DOG\n5     CAT\ndtype: object\n"
     ]
    }
   ],
   "source": [
    "s = pd.Series(['A', 'B', 'C', 'Aaba','DOG','CAT'])\n",
    "print(s.str.lower())\n",
    "print(s.str.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    0\n",
       "3    1\n",
       "4    0\n",
       "5    1\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# counting occurrence of geeks\n",
    "count = s.str.count(\"A\")\n",
    "  \n",
    "# display\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original DataFrames:\n  student_id              name  marks\n0         S1  Danniella Fenton    200\n1         S2      Ryder Storey    210\n2         S3      Bryce Jensen    190\n3         S4         Ed Bernal    222\n4         S5       Kwame Morin    199\n-------------------------------------\n  student_id              name  marks\n0         S4  Scarlette Fisher    201\n1         S5  Carla Williamson    200\n2         S6       Dante Morse    198\n3         S7    Kaiser William    219\n4         S8   Madeeha Preston    201\n\nJoin the said two dataframes along rows:\n  student_id              name  marks\n0         S1  Danniella Fenton    200\n1         S2      Ryder Storey    210\n2         S3      Bryce Jensen    190\n3         S4         Ed Bernal    222\n4         S5       Kwame Morin    199\n0         S4  Scarlette Fisher    201\n1         S5  Carla Williamson    200\n2         S6       Dante Morse    198\n3         S7    Kaiser William    219\n4         S8   Madeeha Preston    201\n"
     ]
    }
   ],
   "source": [
    "student_data1 = pd.DataFrame({\n",
    "        'student_id': ['S1', 'S2', 'S3', 'S4', 'S5'],\n",
    "         'name': ['Danniella Fenton', 'Ryder Storey', 'Bryce Jensen', 'Ed Bernal', 'Kwame Morin'], \n",
    "        'marks': [200, 210, 190, 222, 199]})\n",
    "\n",
    "student_data2 = pd.DataFrame({\n",
    "        'student_id': ['S4', 'S5', 'S6', 'S7', 'S8'],\n",
    "        'name': ['Scarlette Fisher', 'Carla Williamson', 'Dante Morse', 'Kaiser William', 'Madeeha Preston'], \n",
    "        'marks': [201, 200, 198, 219, 201]})\n",
    "\n",
    "print(\"Original DataFrames:\")\n",
    "print(student_data1)\n",
    "print(\"-------------------------------------\")\n",
    "print(student_data2)\n",
    "print(\"\\nJoin the said two dataframes along rows:\")\n",
    "result_data = pd.concat([student_data1, student_data2])\n",
    "print(result_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original DataFrames:\n  student_id              name  marks\n0         S1  Danniella Fenton    200\n1         S2      Ryder Storey    210\n2         S3      Bryce Jensen    190\n3         S4         Ed Bernal    222\n4         S5       Kwame Morin    199\n-------------------------------------\n  student_id              name  marks\n0         S4  Scarlette Fisher    201\n1         S5  Carla Williamson    200\n2         S6       Dante Morse    198\n3         S7    Kaiser William    219\n4         S8  Madeeha, Preston    201\n\nJoin the said two dataframes along columns:\n  student_id              name  marks student_id              name  marks\n0         S1  Danniella Fenton    200         S4  Scarlette Fisher    201\n1         S2      Ryder Storey    210         S5  Carla Williamson    200\n2         S3      Bryce Jensen    190         S6       Dante Morse    198\n3         S4         Ed Bernal    222         S7    Kaiser William    219\n4         S5       Kwame Morin    199         S8  Madeeha, Preston    201\n"
     ]
    }
   ],
   "source": [
    "student_data1 = pd.DataFrame({\n",
    "        'student_id': ['S1', 'S2', 'S3', 'S4', 'S5'],\n",
    "         'name': ['Danniella Fenton', 'Ryder Storey', 'Bryce Jensen', 'Ed Bernal', 'Kwame Morin'], \n",
    "        'marks': [200, 210, 190, 222, 199]})\n",
    "\n",
    "student_data2 = pd.DataFrame({\n",
    "        'student_id': ['S4', 'S5', 'S6', 'S7', 'S8'],\n",
    "        'name': ['Scarlette Fisher', 'Carla Williamson', 'Dante Morse', 'Kaiser William', 'Madeeha, Preston'], \n",
    "        'marks': [201, 200, 198, 219, 201]})\n",
    "\n",
    "print(\"Original DataFrames:\")\n",
    "print(student_data1)\n",
    "print(\"-------------------------------------\")\n",
    "print(student_data2)\n",
    "print(\"\\nJoin the said two dataframes along columns:\")\n",
    "result_data = pd.concat([student_data1, student_data2], axis = 1)\n",
    "print(result_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   school class           name date_Of_Birth   age  height  weight  address\nS2   s002     V    Kanika Goel     17/05/2002   12     192      32  street2\nS3   s003    VI     Richa Jain     16/02/1999   13     186      33  street3\nS4   s001    VI     Ananya Jha     25/09/1998   13     167      30  street1\nS5   s002     V  Deepika Gupta     11/05/2002   14     151      31  street2\nS6   s004    VI  Karishma Khan     15/09/1997   12     159      32  street4\n\nSplit the said data on school_code wise:\n\nGroup:\ns001\n   school class        name date_Of_Birth   age  height  weight  address\nS4   s001    VI  Ananya Jha     25/09/1998   13     167      30  street1\n\nGroup:\ns002\n   school class           name date_Of_Birth   age  height  weight  address\nS2   s002     V    Kanika Goel     17/05/2002   12     192      32  street2\nS5   s002     V  Deepika Gupta     11/05/2002   14     151      31  street2\n\nGroup:\ns003\n   school class        name date_Of_Birth   age  height  weight  address\nS3   s003    VI  Richa Jain     16/02/1999   13     186      33  street3\n\nGroup:\ns004\n   school class           name date_Of_Birth   age  height  weight  address\nS6   s004    VI  Karishma Khan     15/09/1997   12     159      32  street4\n\nType of the object:\n<class 'pandas.core.groupby.generic.DataFrameGroupBy'>\n"
     ]
    }
   ],
   "source": [
    "student_data = pd.DataFrame({\n",
    "    'school': ['s002','s003','s001','s002','s004'],\n",
    "    'class': ['V', 'VI', 'VI', 'V', 'VI'],\n",
    "    'name': ['Kanika Goel','Richa Jain','Ananya Jha', 'Deepika Gupta', 'Karishma Khan'],\n",
    "    'date_Of_Birth ': ['17/05/2002','16/02/1999','25/09/1998','11/05/2002','15/09/1997'],\n",
    "    'age': [12, 13, 13, 14, 12],\n",
    "    'height': [192, 186, 167, 151, 159],\n",
    "    'weight': [32, 33, 30, 31, 32],\n",
    "    'address': ['street2', 'street3', 'street1', 'street2', 'street4']},\n",
    "    index=['S2', 'S3', 'S4', 'S5', 'S6'])\n",
    "print(student_data)\n",
    "\n",
    "print('\\nSplit the said data on school_code wise:')\n",
    "result = student_data.groupby(['school'])\n",
    "for name,group in result:\n",
    "    print(\"\\nGroup:\")\n",
    "    print(name)\n",
    "    print(group)\n",
    "print(\"\\nType of the object:\")\n",
    "print(type(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_data.to_csv('file_student.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.0.7-py2.py3-none-any.whl (243 kB)\n",
      "Collecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.0.1.tar.gz (8.4 kB)\n",
      "Building wheels for collected packages: et-xmlfile\n",
      "  Building wheel for et-xmlfile (setup.py): started\n",
      "  Building wheel for et-xmlfile (setup.py): finished with status 'done'\n",
      "  Created wheel for et-xmlfile: filename=et_xmlfile-1.0.1-py3-none-any.whl size=8913 sha256=7442da42b4dbfb6d5db81519c653544fce1ac6b6a8bfd7f681769c3023563587\n",
      "  Stored in directory: c:\\users\\anany\\appdata\\local\\pip\\cache\\wheels\\bb\\9b\\ff\\8fdf015c95a57224237bf88fbb9e45237cbe8177213bc13732\n",
      "Successfully built et-xmlfile\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.0.1 openpyxl-3.0.7\n",
      "WARNING: You are using pip version 20.2.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\python39\\python.exe -m pip install --upgrade pip' command.\n",
      "DataFrame is written to Excel File successfully.\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl\n",
    "file_name = 'StudentData.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "student_data.to_excel(file_name, sheet_name='Sheet1')\n",
    "print('DataFrame is written to Excel File successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'tables'.  Use pip or conda to install tables.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-ba79ec758244>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#!pip install tables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mstudent_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_hdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'student_data.h5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'df'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_hdf\u001b[1;34m(self, path_or_buf, key, mode, complevel, complib, append, format, index, min_itemsize, nan_rep, dropna, data_columns, errors, encoding)\u001b[0m\n\u001b[0;32m   2604\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpytables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2606\u001b[1;33m         pytables.to_hdf(\n\u001b[0m\u001b[0;32m   2607\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2608\u001b[0m             \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\pandas\\io\\pytables.py\u001b[0m in \u001b[0;36mto_hdf\u001b[1;34m(path_or_buf, key, value, mode, complevel, complib, append, format, index, min_itemsize, nan_rep, dropna, data_columns, errors, encoding)\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[0mpath_or_buf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstringify_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m         with HDFStore(\n\u001b[0m\u001b[0;32m    278\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomplevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcomplevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomplib\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcomplib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         ) as store:\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\pandas\\io\\pytables.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, mode, complevel, complib, fletcher32, **kwargs)\u001b[0m\n\u001b[0;32m    540\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"format is not a defined argument for HDFStore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mtables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_optional_dependency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tables\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcomplib\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcomplib\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_complibs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\pandas\\compat\\_optional.py\u001b[0m in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, raise_on_missing, on_version)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mraise_on_missing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Missing optional dependency 'tables'.  Use pip or conda to install tables."
     ]
    }
   ],
   "source": [
    "#!pip install tables\n",
    "student_data.to_hdf('student_data.h5', key='df', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}